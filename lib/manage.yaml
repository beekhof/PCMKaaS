heat_template_version: 2013-05-23

description: Template that installs a small example web server.

parameters:
  prefix:
    type: string
    label: Cluster node name
    description: Stem from which to create a node name
    default: cluster
  offset:
    type: number
    label: Cluster node number
    description: Number of this instance in the cluster
    default: 0
  cluster_size:
    type: number
    label: Cluster size
    description: Number of instances in cluster.
    default: 3
  servers:
    type: comma_delimited_list
    label: Servers
    description: Comma separated list of private IPs in the cluster.
  server_names:
    type: comma_delimited_list
    label: Servers
    description: Comma separated list of cluster server names.
  image:
    type: string
    label: Image name or ID
    description: Image to be used for server. Please use an Ubuntu based image.
    default: trusty-server-cloudimg-amd64
  flavor:
    type: string
    label: Flavor
    description: Type of instance (flavor) to be used on the compute instance.
    default: m1.tiny
  key:
    type: string
    label: Key name
    description: Name of key-pair to be installed on the management instance.
    default: default
  cluster_key:
    type: string
    label: Key name
    description: Name of key-pair to for accessing cluster instances.
    default: default
  public_network:
    type: string
    label: Private network name or ID
    description: Network to attach server to.
    default: private
  private_network:
    type: string
    label: Private network name or ID
    description: Network to attach server to.
    default: private
  security_group:
    type: string
    description: Public network security group

resources:
  signal_handle:
    type: "OS::Heat::SwiftSignalHandle"

  wait_on_server:
    type: OS::Heat::SwiftSignal
    properties:
      handle: {get_resource: signal_handle}
      count: 1
      timeout: 600

  floating_ip:
    type: floating_ip.yaml
    properties:
      public_network: { get_param: public_network }
      private_network: { get_param: private_network }
      security_group: { get_param: security_group }

  management_node:
    type: OS::Nova::Server
    properties:
      name: { "Fn::Join": [ '-', [ { get_param: prefix }, 'manage' ]] }
      image: { get_param: image }
      flavor: { get_param: flavor }
      key_name: { get_param: key }
      metadata:
        servers: { get_param: servers }
        server_names: { get_param: server_names }
      networks:
        - port: { get_attr: [floating_ip, port] }
      user_data_format: RAW
      user_data:
        str_replace:
          params:
            # Replace all occurances of "wc_notify" in the script with an
            # appropriate curl PUT request using the "curl_cli" attribute
            # of the SwiftSignalHandle resource
            wc_notify:   { get_attr: ['signal_handle', 'curl_cli'] }
            __pass__:    { get_param: prefix }
            __prefix__:  { get_param: prefix }
            __count__:   { get_param: cluster_size }
            __key__:     { get_param: cluster_key }
            __store__:   '/usr/share/pacemaker'
          template: |
            #!/bin/bash -ex

            /usr/bin/logger "prep: begin __key__"

            # Save the private key for accessing the cluster
            mkdir /root/.ssh
            echo "__key__" > /root/.ssh/id_rsa
            chmod 600 /root/.ssh/id_rsa

            ls -alR /home

            # Write and call the update script
            cat >>__store__/update.py <<EOF
            import sys
            import json
            import subprocess
            
            # load server list from metadata
            sfile = '__store__/servers.json'
            metadata = json.loads(sys.stdin.read())
            new_servers = json.loads(metadata.get('meta', {}).get('servers', '[]'))
            if not new_servers:
                sys.exit(1)  # bad metadata
            
            # compare against known list of servers
            current_servers = json.loads(open(sfile).read())
            if current_servers == new_servers:
                sys.exit(0)  # no changes
            
            # record updated list of servers
            open(sfile, 'wt').write(json.dumps(new_servers))
            
            # Re-create /etc/hosts
            f = open('/etc/hosts', 'wt')
            f.write('127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4')
            f.write('::1         localhost localhost.localdomain localhost6 localhost6.localdomain6')
            for i, server in enumerate(new_servers):
                f.write('{0} __prefix__-{1} __prefix__-{1}.localdomain\n'.format(server,i,i))
            f.close()
            f = open('__store__/nodelist.pcs', 'wt')
            for i, server in enumerate(new_servers):
                f.write('__prefix__-{0}@{1}'.format(i,server))
            f.close()
            EOF

            echo "[]" > __store__/servers.json
            curl -s http://169.254.169.254/openstack/latest/meta_data.json | python __store__/update.py

            # add a cron job to monitor the metadata and update /etc/hosts
            #crontab -l >_crontab || true
            #echo "* * * * * curl -s http://169.254.169.254/openstack/latest/meta_data.json | python __store__/update.py | /usr/bin/logger -t pcs_update" >>_crontab
            #crontab <_crontab
            #rm _crontab

            # Sync the hosts file
            for IP in ${node_names}; do
                scp /etc/hosts cloud-user@${IP}:
                ssh -t cloud-user@${IP} -- sudo cp ./hosts /etc/hosts
            done

            # Potentially, if we ever support dynamic resizing, the short name may not be stable enough
            # and we'll have to use IPs to create/update the cluster instead
            node_names=$(cat /etc/hosts | grep __prefix__ | awk '{print $3}' | tr '\n' ' ' )
            node_ips=$(cat /etc/hosts | grep __prefix__ | awk '{print $1}' | tr '\n' ' ' )

            # Switch to CentOS so we can install some packages 
            rm -rf /usr/share/doc/redhat-release
            rm -rf /usr/share/redhat-release
            rpm -Uvh --force http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-2.1511.el7.centos.2.10.x86_64.rpm

            # Notify success early during development phase
            wc_notify --data-binary '{"status": "SUCCESS", "data": "Script execution succeeded"}'

            # Sync and install the packages
            yum --releasever=7 distro-sync -y
            yum install -y pcs pacemaker corosync fence-agents-all resource-agents ntp

            # autheticate nodes, requires all nodes to have pcsd up and running 
            # the -p option is used to give the password on command line and make it easier to script
            pcs cluster auth $node_names -u hacluster -p __pass__
            
            # Construct and start the cluster
            pcs cluster setup --start --enable --name __prefix__  ${node_names}

            # Assuming long running operation completed successfully, notify success signal
            wc_notify --data-binary '{"status": "SUCCESS", "data": "Script execution succeeded"}'
  
            # Alternatively if operation fails a FAILURE with reason and data may be sent,
            # notify failure signal example below
            # wc_notify --data-binary '{"status": "FAILURE", "reason":"Operation failed due to xyz error", "data":"Script execution failed"}'

outputs:
  name:
    description: Name of the tiny instance.
    value: { get_attr: [management_node, name] }
  ip:
    description: The IP address of the tiny instance.
    value: { get_attr: [management_node, first_address] }
  public_ip:
    description: The public IP address of the tiny instance.
    value: { get_attr: [floating_ip, ip] }
