heat_template_version: 2013-05-23

description: Template that installs a small example web server.

parameters:
  image:
    type: string
    label: Image name or ID
    description: Image to be used for server. Please use an Ubuntu based image.
    default: trusty-server-cloudimg-amd64
  flavor:
    type: string
    label: Flavor
    description: Type of instance (flavor) to be used on the compute instance.
    default: m1.tiny
  key:
    type: string
    label: Key name
    description: Name of key-pair to be installed on the management instance.
    default: default
  cluster_key:
    type: string
    label: Key name
    description: Name of key-pair to for accessing cluster instances.
    default: default
  public_network:
    type: string
    label: Private network name or ID
    description: Network to attach server to.
    default: private
  private_network:
    type: string
    label: Private network name or ID
    description: Network to attach server to.
    default: private
  security_group:
    type: string
    description: Public network security group
  prefix:
    type: string
    label: Cluster name
    description: Stem from which to create a node name
    default: cluster
  cluster_pass:
    type: string
    label: Cluster password
  servers:
    type: comma_delimited_list
    label: Servers
    description: Comma separated list of private IPs in the cluster.
  server_names:
    type: comma_delimited_list
    label: Servers
    description: Comma separated list of cluster server names.

resources:
  signal_handle:
    type: "OS::Heat::SwiftSignalHandle"

  wait_on_server:
    type: OS::Heat::SwiftSignal
    properties:
      handle: {get_resource: signal_handle}
      count: 1
      timeout: 2000

  floating_ip:
    type: floating_ip.yaml
    properties:
      public_network: { get_param: public_network }
      private_network: { get_param: private_network }
      security_group: { get_param: security_group }

  management_node:
    type: OS::Nova::Server
    properties:
      name: { "Fn::Join": [ '-', [ { get_param: prefix }, 'manage' ]] }
      image: { get_param: image }
      flavor: { get_param: flavor }
      key_name: { get_param: key }
      metadata:
        servers: { get_param: servers }
        server_names: { get_param: server_names }
      networks:
        - port: { get_attr: [floating_ip, port] }
      user_data_format: RAW
      user_data:
        str_replace:
          params:
            # Replace all occurances of "wc_notify" in the script with an
            # appropriate curl PUT request using the "curl_cli" attribute
            # of the SwiftSignalHandle resource
            wc_notify:   { get_attr: ['signal_handle', 'curl_cli'] }
            __pass__:    { get_param: cluster_pass }
            __prefix__:  { get_param: prefix }
            __key__:     { get_param: cluster_key }
            __store__:   '/usr/share/'
          template: |
            Content-Type: multipart/mixed; boundary="===============3343034662225461311=="
            MIME-Version: 1.0
            
            --===============3343034662225461311==
            MIME-Version: 1.0
            Content-Type: text/cloud-config; charset="us-ascii"
            Content-Transfer-Encoding: 7bit
            Content-Disposition: attachment; filename="cloud.config"
            
            #cloud-config
            # package_upgrade: true
            packages:
              - pcs

            --===============3343034662225461311==
            MIME-Version: 1.0
            Content-Type: text/x-shellscript; charset="us-ascii"
            Content-Transfer-Encoding: 7bit
            Content-Disposition: attachment; filename="cloud.sh"
            
            #!/bin/sh -ex

            # Save the private key for accessing the cluster
            echo "__key__" > /root/.ssh/id_rsa
            chmod 600 /root/.ssh/id_rsa

            cp /root/.ssh/id_rsa /home/centos/.ssh/id_rsa
            chmod 600 /home/centos/.ssh/id_rsa
            chown centos /home/centos/.ssh/id_rsa

            # Write and call the update script
            cat >>__store__/update.py <<EOF
            import sys
            import json
            import subprocess
            
            # load server list from metadata
            sfile = '__store__/servers.json'
            metadata = json.loads(sys.stdin.read())
            new_servers = json.loads(metadata.get('meta', {}).get('servers', '[]'))
            new_server_names = json.loads(metadata.get('meta', {}).get('server_names', '[]'))
            if not new_servers:
                sys.exit(1)  # bad metadata
            
            # compare against known list of servers
            current_servers = json.loads(open(sfile).read())
            if current_servers == new_servers:
                sys.exit(0)  # no changes
            
            # record updated list of servers
            open(sfile, 'wt').write(json.dumps(new_servers))

            # Re-create /etc/hosts
            f = open('/etc/hosts', 'wt')
            f.write('127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n')
            f.write('::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n')
            for i, server in enumerate(new_servers):
                f.write('{0} {1} {1}.localdomain\n'.format(server,new_server_names[i],new_server_names[i]))
            f.close()
            f = open('__store__/nodelist.pcs', 'wt')
            for i, server in enumerate(new_servers):
                f.write('-{0}@{1}'.format(new_server_names[i],server))
            f.close()
            EOF

            echo "[]" > __store__/servers.json
            curl -s http://169.254.169.254/openstack/latest/meta_data.json | python __store__/update.py

            # add a cron job to monitor the metadata and update /etc/hosts
            #crontab -l >_crontab || true
            #echo "* * * * * curl -s http://169.254.169.254/openstack/latest/meta_data.json | python __store__/update.py | /usr/bin/logger -t pcs_update" >>_crontab
            #crontab <_crontab
            #rm _crontab

            node_names=$(cat /etc/hosts | grep __prefix__ | awk '{print $3}' | tr '\n' ' ' )
            node_ips=$(cat /etc/hosts | grep __prefix__ | awk '{print $1}' | tr '\n' ' ' )

            # Sync the hosts file
            for IP in ${node_names}; do
                scp /etc/hosts centos@${IP}:
                ssh -t centos@${IP} -- sudo cp ./hosts /etc/hosts
            done

            # autheticate nodes, requires all nodes to have pcsd up and running 
            # the -p option is used to give the password on command line and make it easier to script
            pcs cluster auth $node_names -u hacluster -p __pass__
            
            # Construct and start the cluster
            pcs cluster setup --start --enable --name __prefix__  ${node_names}

            # Assuming long running operation completed successfully, notify success signal
            wc_notify --data-binary '{"status": "SUCCESS", "data": "Script execution succeeded"}'

            find /var/lib/cloud/ -type f -print -exec cat \{\} \;
            
            # Alternatively if operation fails a FAILURE with reason and data may be sent,
            # notify failure signal example below
            # wc_notify --data-binary '{"status": "FAILURE", "reason":"Operation failed due to xyz error", "data":"Script execution failed"}'
          
            --===============3343034662225461311==--

outputs:
  name:
    description: Name of the tiny instance.
    value: { get_attr: [management_node, name] }
  ip:
    description: The IP address of the tiny instance.
    value: { get_attr: [management_node, first_address] }
  public_ip:
    description: The public IP address of the tiny instance.
    value: { get_attr: [floating_ip, ip] }
